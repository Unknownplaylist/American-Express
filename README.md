# AmEx XGBoost - Windows Version

**Pure Windows-compatible version with automated Kaggle pipeline and intelligent caching**

## Quick Start

### 1. Setup
```bash
# Install dependencies
pip install -r requirements.txt

# Check Kaggle setup
python check_kaggle_setup.py
```

### 2. Add Data
Place CSV files in `data/` directory:
- `train_data.csv`
- `test_data.csv`
- `train_labels.csv`
- `sample_submission.csv`

### 3. Choose Your Workflow

#### **ğŸ§ª Test Pipeline (2 minutes)**
```bash
python run_test.py
```

#### **ğŸ¯ Full Training (1-2 hours)**
```bash
python train_full.py
```

#### **ğŸ¤– Automated Kaggle Pipeline (Continuous)**
```bash
python run_auto_pipeline.py
```

## ğŸ“ Core Files

### **Training & Pipeline**
- `train_full.py` - Complete training with cross-validation
- `auto_kaggle_pipeline.py` - Automated training + Kaggle submission + tuning
- `run_auto_pipeline.py` - Easy interface for automated pipeline

### **Caching System**
- `cached_feature_engineering.py` - Smart feature caching (4x speed boost)
- `manage_cache.py` - Cache management utility
- `feature_cache/` - Cached features directory

### **Core Components**
- `config.py` - Windows-optimized XGBoost parameters
- `simple_feature_engineering.py` - Pure pandas feature engineering
- `test_small_batch_cpu.py` - Windows test script

### **Utilities**
- `check_kaggle_setup.py` - Verify Kaggle API configuration
- `quick_submit.py` - Manual Kaggle submission
- `requirements.txt` - Windows-only dependencies

## ğŸš€ Intelligent Caching System

### **Speed Improvements:**
- **First Run**: ~20 minutes (feature engineering + training)
- **Subsequent Runs**: ~5 minutes (cached features + training)
- **4x faster** iterations for auto-tuning

### **Cache Management:**
```bash
# List cached features
python manage_cache.py list

# Clear cache (if data changes)
python manage_cache.py clear

# Force rebuild cache
python manage_cache.py rebuild

# Test caching system
python manage_cache.py test
```

### **How It Works:**
- Automatically detects data file changes
- Saves engineered features as compressed pickles
- Smart hash validation prevents stale cache
- Resume-friendly for interrupted runs

## ğŸ¤– Automated Kaggle Pipeline

### **What It Does:**
1. **Trains** XGBoost with different parameters
2. **Submits** predictions to Kaggle automatically
3. **Gets scores** and tracks performance
4. **Tunes parameters** based on results
5. **Repeats** until target score reached

### **Auto-Tuning Strategy:**
- **Score < 0.790**: Aggressive tuning (learning rate, depth)
- **Score 0.790-0.800**: Fine-tuning (regularization)
- **Score > 0.800**: Micro-optimization

### **Configuration:**
- **Target Score**: 0.805 (top leaderboard performance)
- **Max Iterations**: 15
- **Auto-saves**: Best models and progress

## ğŸ”§ Setup Kaggle API

### **1. Get Credentials:**
- Go to https://www.kaggle.com/account
- Click "Create New API Token"
- Download `kaggle.json`

### **2. Install Credentials:**
- Place in: `C:\Users\YourName\.kaggle\kaggle.json`
- Or run: `python check_kaggle_setup.py` for guidance

### **3. Join Competition:**
- Visit: https://www.kaggle.com/competitions/amex-default-prediction
- Click "Join Competition"

### **4. Verify Setup:**
```bash
python check_kaggle_setup.py
```

## ğŸ“Š Expected Performance

### **Test Results:**
- **Features**: 1,107 engineered features
- **Validation Score**: ~0.759 (small batch)
- **Training Time**: 2-5 minutes per iteration

### **Full Training Results:**
- **Cross-Validation**: 5-fold with AmEx metric
- **Expected Score**: 0.790-0.805
- **Auto-Tuning**: Continuous improvement

### **Pipeline Iterations:**
- **Iteration 1**: Baseline (~0.790-0.795)
- **Iterations 2-5**: Gradual improvement
- **Iterations 6+**: Fine-tuning for optimal score

## ğŸ’¡ Usage Patterns

### **Quick Testing:**
```bash
python run_test.py              # 2 minutes - verify setup
```

### **One-Time Training:**
```bash
python train_full.py            # 1-2 hours - complete training
python quick_submit.py          # Submit to Kaggle
```

### **Automated Optimization:**
```bash
python run_auto_pipeline.py     # Continuous - hands-off optimization
```

### **Cache Management:**
```bash
python manage_cache.py list     # Check cached features
python manage_cache.py clear    # Reset cache if needed
```

## ğŸ¯ Key Features

- **âœ… Windows Compatible**: Pure pandas, no GPU dependencies
- **ğŸš€ Intelligent Caching**: 4x faster subsequent runs
- **ğŸ¤– Auto-Tuning**: Hands-off parameter optimization
- **ğŸ“Š Kaggle Integration**: Automated submission and scoring
- **ğŸ’¾ Resume-Friendly**: Save/restore progress automatically
- **ğŸ”§ Easy Setup**: Minimal configuration required

## ğŸ† Production Ready

**Status: Fully operational automated ML pipeline**

- Proven feature engineering (1,107 features)
- Robust cross-validation with AmEx metric
- Automated hyperparameter tuning
- Intelligent caching for efficiency
- Complete Kaggle integration

- Windows-optimized for reliability
